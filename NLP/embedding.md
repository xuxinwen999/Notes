## Embedding 生成
### BERT的Embedding生成机制
* CLS向量：使用[CLS]标记对应的向量作为句子表征，通常用于seq层面的任务。
* average pooling*：对最后一层所有token的向量取均值，保留全局语义信息，通用性较强（很多实践证明强于CLS）
* 动态组合：拼接或加权最后若干层（如4层）的输出向量，融合不同层级的语义特征。
	
2⃣️BGE的向量优化技术
—模型架构：基于BERT改进的检索专用模型，针对语义匹配优化。
—核心优化点：
—对比学习训练：通过正负样本对优化向量空间，增强相似文本的聚集性和非相似文本的区分度。
—指令微调：支持添加任务指令（如“生成检索向量”），提升对特定任务的适配能力。
—向量归一化：默认输出L2归一化后的向量，直接支持余弦相似度计算，减少计算开销。
	
3⃣️GPT的Embedding生成方案
—模型架构：基于单向Transformer解码器，生成自回归式的向量表示。
—生成方法：
—末层隐状态：取最后一个token的向量或全局平均池化，但因缺乏双向上下文，语义表征能力较弱。
—下游任务微调：通过分类或相似度任务微调模型，提取任务相关的向量表示。
局限性：GPT的Embedding在检索或匹配任务中通常弱于BERT/BGE等双向模型。
	
4⃣️通用大模型（如Qwen/LLaMA）的Embedding优化
—模型改造方法：
—结构改进：在Transformer顶层添加池化层（如Mean/Max Pooling或CLS标记），将token序列整合为定长向量。
—训练优化：采用对比学习目标（如Triplet Loss、InfoNCE）或检索任务微调，结合负样本排序提升区分度。
	
最后，大模型Embedding应用注意事项
—文本长度：超长文本需截断或分段处理；可尝试滑动窗口或层次化池化减少信息损失
—领域适配：通用模型在专业领域表现可能不佳，建议通过领域数据微调（如医疗/法律语料）
—向量归一化：相似度计算时需统一做L2归一化，避免向量模长影响结果（尤其针对内积计算）