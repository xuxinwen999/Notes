# GPT系列报告演变总结
## GPT-1
**Key Points**:<br>
1. 提出generative & task-agnostic pre-training以利用substantial unlabelled corpus;
2. 针对"The goal of learning a universal representation that transfers with little adaptation to a wide range of tasks", 证明generative pre-train + little fine-tuning 在实验任务上甚至超过sota的定制模型；
3. Zero-shot探索：analyzed zero-shot behaviors of the pre-trained model on four different settings and demonstrate that it acquires useful linguistic knowledge for downstream tasks.